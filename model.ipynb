{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel, AutoConfig, AutoModel, AutoProcessor, AutoFeatureExtractor, AutoTokenizer, TrOCRProcessor, AutoModelForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "\n",
    "def get_processor(encoder_name, decoder_name):\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(decoder_name)\n",
    "    processor = TrOCRProcessor(feature_extractor, tokenizer)\n",
    "    return processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(encoder_name, decoder_name):\n",
    "\n",
    "    encoder_config = AutoConfig.from_pretrained(encoder_name)\n",
    "    encoder_config.is_decoder = False\n",
    "    encoder_config.add_cross_attention = False\n",
    "    encoder = AutoModel.from_config(encoder_config)\n",
    "\n",
    "\n",
    "    decoder_config = AutoConfig.from_pretrained(decoder_name)\n",
    "    decoder_config.is_decoder = True\n",
    "    decoder_config.add_cross_attention=True\n",
    "    decoder = AutoModelForCausalLM.from_config(decoder_config)\n",
    "\n",
    "    config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config)\n",
    "    model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder, config=config)\n",
    "\n",
    "    processor = get_processor(encoder_name, decoder_name)\n",
    "\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.vocab_size = model.config.decoder.vocab_size\n",
    "    \n",
    "    model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "    model.config.max_length = 100\n",
    "    model.config.early_stopping = True\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model.config.length_penalty = 2.0\n",
    "    model.config.num_beams = 4\n",
    "\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, processor = get_model('facebook/deit-tiny-patch16-224', 'cl-tohoku/bert-base-japanese-char-v2')\n",
    "\n",
    "# model, processor = get_model('google/vit-base-patch16-224', 'cl-tohoku/bert-base-japanese-char-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "wer = load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predicted = logits.argmax(-1)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\n",
    "    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    return {\"wer_score\": wer_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 1490\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 166\n",
      "})\n",
      "tensor([   2,  879,  933,  892,  896,    1, 3464, 3614,  882,  922, 1026, 1026,\n",
      "           1,  869,  933,  892, 2561, 3539,  898,  885,  861,    1,  918,  885,\n",
      "         888,  933,  896,  893,  941, 1026,    3,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:136: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\medal\\Documents\\Programs\\ocr\\wandb\\run-20240303_204139-dtdptfif</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vincent55/huggingface/runs/dtdptfif' target=\"_blank\">ruby-haze-18</a></strong> to <a href='https://wandb.ai/vincent55/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vincent55/huggingface' target=\"_blank\">https://wandb.ai/vincent55/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vincent55/huggingface/runs/dtdptfif' target=\"_blank\">https://wandb.ai/vincent55/huggingface/runs/dtdptfif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2805 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      " 18%|█▊        | 500/2805 [08:16<37:47,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9585, 'grad_norm': 0.861111581325531, 'learning_rate': 4.114081996434938e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "\n",
      " 18%|█▊        | 500/2805 [10:45<37:47,  1.02it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "Removed shared tensor {'decoder.cls.predictions.decoder.weight', 'decoder.cls.predictions.decoder.bias'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6595956087112427, 'eval_wer_score': 1.1269698334083746, 'eval_runtime': 148.0841, 'eval_samples_per_second': 1.121, 'eval_steps_per_second': 0.142, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1000/2805 [19:00<29:12,  1.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6315, 'grad_norm': 0.7005841135978699, 'learning_rate': 3.222816399286987e-05, 'epoch': 5.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 36%|███▌      | 1000/2805 [20:05<29:12,  1.03it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6298285126686096, 'eval_wer_score': 1.1508329581269698, 'eval_runtime': 64.269, 'eval_samples_per_second': 2.583, 'eval_steps_per_second': 0.327, 'epoch': 5.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1500/2805 [28:15<20:05,  1.08it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5426, 'grad_norm': 1.022025227546692, 'learning_rate': 2.3315508021390376e-05, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 53%|█████▎    | 1500/2805 [29:29<20:05,  1.08it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6239275336265564, 'eval_wer_score': 1.158937415578568, 'eval_runtime': 73.867, 'eval_samples_per_second': 2.247, 'eval_steps_per_second': 0.284, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 2000/2805 [44:19<08:50,  1.52it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4455, 'grad_norm': 1.1543413400650024, 'learning_rate': 1.4402852049910875e-05, 'epoch': 10.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 71%|███████▏  | 2000/2805 [46:13<08:50,  1.52it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6420084834098816, 'eval_wer_score': 1.1580369203061684, 'eval_runtime': 114.5584, 'eval_samples_per_second': 1.449, 'eval_steps_per_second': 0.183, 'epoch': 10.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 2500/2805 [53:21<05:16,  1.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.378, 'grad_norm': 0.9236273169517517, 'learning_rate': 5.490196078431373e-06, 'epoch': 13.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 89%|████████▉ | 2500/2805 [55:18<05:16,  1.04s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6490842700004578, 'eval_wer_score': 1.1391265195857723, 'eval_runtime': 116.9775, 'eval_samples_per_second': 1.419, 'eval_steps_per_second': 0.18, 'epoch': 13.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2805/2805 [1:00:03<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3604.5989, 'train_samples_per_second': 6.2, 'train_steps_per_second': 0.778, 'train_loss': 0.7428336729977858, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2805, training_loss=0.7428336729977858, metrics={'train_runtime': 3604.5989, 'train_samples_per_second': 6.2, 'train_steps_per_second': 0.778, 'train_loss': 0.7428336729977858, 'epoch': 15.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "run_name='debug',\n",
    "max_len=100,\n",
    "num_decoder_layers=2,\n",
    "batch_size=10,\n",
    "num_epochs=5,\n",
    "fp16=True,\n",
    "\n",
    "#train_ds = load_dataset(\"imagefolder\", data_dir=\"./dataset/\", split=\"train\")\n",
    "#test_ds = load_dataset(\"imagefolder\", data_dir=\"./dataset/\", split=\"validation\")\n",
    "\n",
    "ds = load_dataset(\"imagefolder\", data_dir=\"./manga_dataset/\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "\n",
    "# ds = load_dataset(\"lambdalabs/pokemon-blip-captions\")\n",
    "# ds = ds[\"train\"].train_test_split(test_size=0.1) #{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x1280 at 0x1AE95F68E50>, 'text': 'a yellow and black cartoon character with a big smile'}\n",
    "# train_ds = ds[\"train\"]\n",
    "# test_ds = ds[\"test\"]\n",
    "\n",
    "print(train_ds)\n",
    "print(test_ds)\n",
    "\n",
    "# def transforms(example_batch):\n",
    "#     images = [x for x in example_batch[\"image\"]]\n",
    "#     captions = [x for x in example_batch[\"text\"]]\n",
    "#     inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "#     inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "#     return inputs\n",
    "\n",
    "# from torchvision.transforms import (\n",
    "#     CenterCrop,\n",
    "#     Compose,\n",
    "#     Normalize,\n",
    "#     RandomHorizontalFlip,\n",
    "#     RandomResizedCrop,\n",
    "#     Resize,\n",
    "#     ToTensor,\n",
    "# )\n",
    "\n",
    "# train_transforms = Compose(\n",
    "#         [\n",
    "#             RandomResizedCrop(224),\n",
    "#             RandomHorizontalFlip(),\n",
    "#             ToTensor(),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "# def preprocess_train(example_batch):\n",
    "#     \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "#     example_batch[\"pixel_values\"] = [\n",
    "#         train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "#     ]\n",
    "#     return example_batch\n",
    "\n",
    "# def transform(example_batch):\n",
    "#     # Take a list of PIL images and turn them to pixel values\n",
    "#     inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "#     # Don't forget to include the labels!\n",
    "#     inputs['text'] = example_batch['text']\n",
    "#     return inputs\n",
    "\n",
    "# def transform(example_batch):\n",
    "#     # Take a list of PIL images and turn them to pixel values\n",
    "\n",
    "#     inputs = processor.feature_extractor([x for x in example_batch['image']], return_tensors=\"pt\").pixel_values.squeeze()#[None].to(model.device)\n",
    "#     # Don't forget to include the labels!\n",
    "#     #txt = processor.tokenizer(example_batch['text'], padding=\"max_length\", max_length=max_len, truncation=True).input_ids\n",
    "\n",
    "#     # txt = np.array(txt)\n",
    "\n",
    "#     # encoding = {\n",
    "#     #     \"pixel_values\": inputs,\n",
    "#     #     \"text\": torch.tensor(txt),\n",
    "#     # }\n",
    "#     # txt[txt == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "#     # inputs['text'] = torch.tensor(txt)\n",
    "#     inputs['text'] = example_batch['text']\n",
    "#     return inputs\n",
    "\n",
    "def transform(example_batch):\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    inputs['labels'] = torch.tensor(processor.tokenizer(example_batch['text'],\n",
    "                                                    padding=\"max_length\",\n",
    "                                                    max_length=100,\n",
    "                                                    truncation=True).input_ids)\n",
    "                                   #add back np.array()                 \n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "train_ds.set_transform(transform)\n",
    "test_ds.set_transform(transform)\n",
    "\n",
    "print(train_ds[0]['labels'])\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    #per_device_train_batch_size=10,\n",
    "    #per_device_eval_batch_size=10,\n",
    "    fp16=fp16,\n",
    "    fp16_full_eval=fp16,\n",
    "    #dataloader_num_workers=8,\n",
    "    output_dir = Path(\"./output\"),\n",
    "    #logging_steps=10,\n",
    "    #save_steps=20000,\n",
    "    #eval_steps=20000,\n",
    "    num_train_epochs=15,\n",
    "    #run_name=run_name,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "#url = 'https://assets.pokemon.com/assets/cms2/img/pokedex/full/123.png'\n",
    "#image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "path = './manga_dataset/crops/00000d36.png'\n",
    "image = Image.open(path)\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-base-patch16-224')\n",
    "\n",
    "image = image.convert('L').convert('RGB')\n",
    "inputs = feature_extractor(image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "inputs = model.generate(inputs[None].to(model.device), max_length=100)[0].cpu()\n",
    "\n",
    "outputs = processor.tokenizer.decode(inputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "は い お じ い さ ん\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./model1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
